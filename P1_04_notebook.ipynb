{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA6l1GjLpQHJ"
      },
      "source": [
        "# reconnaissance de langue avec Azure Cognitive Service - Text Analytics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ODUD3fopmoA"
      },
      "source": [
        "**colab-env** permet de récupérer mes credentials contenues dans le fichier vars.env sans les exposer dans le notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y0_IlUYlnL3",
        "outputId": "d2316674-7049-4b3c-ecbd-ade1b2f393b2"
      },
      "source": [
        "!pip install colab-env --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab-env\n",
            "  Downloading colab-env-0.2.0.tar.gz (4.7 kB)\n",
            "Collecting python-dotenv<1.0,>=0.10.0\n",
            "  Downloading python_dotenv-0.19.1-py2.py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: colab-env\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colab-env: filename=colab_env-0.2.0-py3-none-any.whl size=3836 sha256=3e0b232e39391fb231a7c25d529813c1fcecf6b56c2ef02cc643bedc872d172d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/ca/e8/3d25b6abb4ac719ecb9e837bb75f2a9b980430005fb12a9107\n",
            "Successfully built colab-env\n",
            "Installing collected packages: python-dotenv, colab-env\n",
            "Successfully installed colab-env-0.2.0 python-dotenv-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gd-S6wpqmPZ"
      },
      "source": [
        "importation des librairies nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2JKmmSrmVg2",
        "outputId": "936bf3d2-3a9a-4ef7-f729-e83ac9933ac7"
      },
      "source": [
        "import colab_env\n",
        "import os\n",
        "import requests\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hSTORuoqxqQ"
      },
      "source": [
        "récupération de la clé API et de l'endpoint pour utiliser le service Cognitive Service - Text Analytics que j'ai créé sur Azure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55YuSkgemcUZ"
      },
      "source": [
        "cog_key = os.getenv(\"COG_KEY\")\n",
        "cog_endpoint = os.getenv(\"COG_ENDPOINT\")\n",
        "language_recognition_endpoint = cog_endpoint + \"/text/analytics/v3.1/languages\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Eu9seE6rYuZ"
      },
      "source": [
        "**recognize_language** est la fonction que j'ai créée pour envoyer une requête **POST** à l'API pour la reconnaissance de langue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlXM9GVZwsYi"
      },
      "source": [
        "def recognize_language(text, id, prettyPrint=False):\n",
        "  payload = json.dumps({\n",
        "      \"documents\": [\n",
        "        {\n",
        "            \"id\": id,\n",
        "          \"text\": text\n",
        "        }\n",
        "      ]\n",
        "  })\n",
        "  headers = {\n",
        "    'Ocp-Apim-Subscription-Key': cog_key,\n",
        "    'Content-Type': 'application/json',\n",
        "    'Accept': 'application/json'\n",
        "  }\n",
        "\n",
        "  response = requests.request(\"POST\", language_recognition_endpoint, headers=headers, data=payload)\n",
        "  if (prettyPrint == False):\n",
        "    return response.text\n",
        "  else:\n",
        "    parsed = json.loads(response.text)\n",
        "    return json.dumps(parsed, indent=4, sort_keys=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYKNz5dbruwd"
      },
      "source": [
        "testons le rapidement avec une **entrée utilisateur**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAqol8uSnz3V",
        "outputId": "1df7488d-c94a-40f5-8c70-3fdc234e4196"
      },
      "source": [
        "text = input(\"write your text here : \")\n",
        "lang = recognize_language(text, \"1\", True)\n",
        "print(lang)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "write your text here : ceci est un test\n",
            "{\n",
            "    \"documents\": [\n",
            "        {\n",
            "            \"detectedLanguage\": {\n",
            "                \"confidenceScore\": 0.95,\n",
            "                \"iso6391Name\": \"fr\",\n",
            "                \"name\": \"French\"\n",
            "            },\n",
            "            \"id\": \"1\",\n",
            "            \"warnings\": []\n",
            "        }\n",
            "    ],\n",
            "    \"errors\": [],\n",
            "    \"modelVersion\": \"2021-01-05\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSMiqsRfmMKf"
      },
      "source": [
        "Pour faciliter les choses par la suite, nous utiliserons la classe **Language_Detection** que j'ai créée, et qui permet :\n",
        "- d'envoyer plus d'un texte dans une seule requête (jusqu'à 1000 sont autorisés par Azure Text Analytics)\n",
        "- de tronquer les textes trop longs pour être envoyés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsFH2rr6iiEP"
      },
      "source": [
        "class Language_Detection:\n",
        "    \"\"\"\n",
        "    Cette classe appelle le service d'analyse de texte d'Azure pour détecter la langue utilisée dans un texte.\n",
        "    \n",
        "    Attributes :\n",
        "        COG_KEY: votre clé API (vous devez l'avoir écrite dans le fichier .env)\n",
        "        COG_ENDPOINT: votre API endpoint (vous devez l'avoir écrite dans le fichier .env)\n",
        "        LANGUAGE_RECOGNITION_URI\n",
        "        HEADERS\n",
        "        MAX_TXT_SIZE\n",
        "        MAX_DOC_AMOUNT\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        load_dotenv()\n",
        "        self.COG_KEY = os.getenv('COG_KEY')\n",
        "        self.COG_ENDPOINT = os.getenv('COG_ENDPOINT')\n",
        "        self.LANGUAGE_RECOGNITION_URI = self.COG_ENDPOINT + \"/text/analytics/v3.1/languages\"\n",
        "        self.HEADERS = {\n",
        "          'Ocp-Apim-Subscription-Key': self.COG_KEY,\n",
        "          'Content-Type': 'application/json',\n",
        "          'Accept': 'application/json'\n",
        "        }\n",
        "        self.MAX_TXT_SIZE = 5120\n",
        "        self.MAX_DOC_AMOUNT = 800\n",
        "        \n",
        "        \n",
        "    def detect(self, text: str, id: str):\n",
        "        '''Utilisez cette fonction pour détecter la langue d'un seul texte\n",
        "        \n",
        "        Paramètres :\n",
        "            text (str):  Le texte dont vous voulez détecter la langue\n",
        "            id (str):    L'identifiant du texte (si c'est juste pour un test et que votre texte n'a pas d'id, passez juste \"1\")\n",
        "        \n",
        "        Retour : \n",
        "            réponse de la requête (response.text)\n",
        "        '''\n",
        "        payload = json.dumps({\n",
        "            \"documents\": [\n",
        "              {\n",
        "                  \"id\": id,\n",
        "                \"text\": self.truncate_big_text(text)\n",
        "              }\n",
        "            ]\n",
        "        })\n",
        "        response = self.call_api(payload)\n",
        "        return response\n",
        "    \n",
        "    \n",
        "    def call_api(self, payload):\n",
        "        '''Pour utiliser cette fonction, vous devez construire votre propre charge utile.\n",
        "        veuillez noter que les limites sont :\n",
        "        - 1000 documents par appel\n",
        "        - 5120 caractères dans un seul document\n",
        "        - 1 Mo par demande\n",
        "        - 100 appels par minute\n",
        "\n",
        "        Paramètres :\n",
        "        payload (str) : string json avec la structure suivante :\n",
        "                {\n",
        "                    \"documents\": [\n",
        "                        {\"id\": id1, \"text\": text 1},\n",
        "                        {\"id\": id2, \"text\": text2},\n",
        "                        ...\n",
        "                        ]\n",
        "                }\n",
        "                \n",
        "        Retour :\n",
        "            réponse de la requête API (response.text)\n",
        "\n",
        "        '''   \n",
        "        response = requests.request(\"POST\", self.LANGUAGE_RECOGNITION_URI, headers=self.HEADERS, data=payload)\n",
        "        return response.text\n",
        "    \n",
        "    \n",
        "    def truncate_big_text(self, text: str):\n",
        "        '''Tronque le texte si sa longueur excède MAX_TXT_SIZE\n",
        "        \n",
        "        Paramètres:\n",
        "            text (str): Le texte à tronquer\n",
        "        '''\n",
        "        if(len(text) > self.MAX_TXT_SIZE):\n",
        "            text = text[:self.MAX_TXT_SIZE - 1]\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAknFp5opckN",
        "outputId": "c01f74dd-3e99-46c9-d1a7-d5dc7e2e53b9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rovekyLo2jWZ",
        "outputId": "05a67e0f-e48e-470d-97b0-69660467ff5b"
      },
      "source": [
        "!ls \"/content/drive/My Drive/OC_Adrien_Godoy/Projet 1/data\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels.csv  urls.txt\tx_train.txt  y_train.txt\n",
            "README.txt  x_test.txt\ty_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8XglErf8ofE"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeSBnbyWOj6m"
      },
      "source": [
        "Liste des langues reconnues par **Azure Text Analytics** ([source](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/language-support?tabs=language-detection)) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfF0-owPItvw"
      },
      "source": [
        "langs = [\"English\", \"Chinese\", \"Standard Chinese\", \"Literary Chinese\", \"Hindi\", \"Spanish\", \"French\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pojCvhMecZNy"
      },
      "source": [
        "Chemin des fichiers que nous utiliserons lors de nos tests.\n",
        "Le dataset WiLI-2018 - Wikipedia Language Identification a été conçu comme un dataset de référence et contient 235 000 paragraphes dans 235 langues. Le dataset est équilibré et une répartition train-test est fournie.\n",
        "\n",
        "Au lieu de travailler avec les 235 langues, je travaille avec 5 langues pour faciliter l'analyse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIwGRUYa_Cfz"
      },
      "source": [
        "base_dir = \"/content/drive/My Drive/OC_Adrien_Godoy/Projet 1/data\"\n",
        "labels_file = base_dir + \"/labels.csv\"\n",
        "x_test_file = base_dir + \"/x_test.txt\"\n",
        "y_test_file = base_dir + \"/y_test.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW0PCFWSdopJ"
      },
      "source": [
        "Construisons un DataFrame avec les données contenues dans le fichier \"labels.csv\", et regardons comment il se présente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "CWlKM_lp_Uya",
        "outputId": "6b5f6a8f-b95e-45fb-b3a5-02dd7ae39eb8"
      },
      "source": [
        "labels_df = pd.read_csv(labels_file, sep=\";\")\n",
        "labels_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>English</th>\n",
              "      <th>Wiki Code</th>\n",
              "      <th>ISO 369-3</th>\n",
              "      <th>German</th>\n",
              "      <th>Language family</th>\n",
              "      <th>Writing system</th>\n",
              "      <th>Remarks</th>\n",
              "      <th>Synonyms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ace</td>\n",
              "      <td>Achinese</td>\n",
              "      <td>ace</td>\n",
              "      <td>ace</td>\n",
              "      <td>Achinesisch</td>\n",
              "      <td>Austronesian</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>afr</td>\n",
              "      <td>Afrikaans</td>\n",
              "      <td>af</td>\n",
              "      <td>afr</td>\n",
              "      <td>Afrikaans</td>\n",
              "      <td>Indo-European</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>als</td>\n",
              "      <td>Alemannic German</td>\n",
              "      <td>als</td>\n",
              "      <td>gsw</td>\n",
              "      <td>Alemannisch</td>\n",
              "      <td>Indo-European</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(ursprünglich nur Elsässisch)</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>amh</td>\n",
              "      <td>Amharic</td>\n",
              "      <td>am</td>\n",
              "      <td>amh</td>\n",
              "      <td>Amharisch</td>\n",
              "      <td>Afro-Asiatic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ang</td>\n",
              "      <td>Old English</td>\n",
              "      <td>ang</td>\n",
              "      <td>ang</td>\n",
              "      <td>Altenglisch</td>\n",
              "      <td>Indo-European</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(ca. 450-1100)</td>\n",
              "      <td>Angelsächsisch</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Label           English  ...                        Remarks        Synonyms\n",
              "0   ace          Achinese  ...                            NaN             NaN\n",
              "1   afr         Afrikaans  ...                            NaN             NaN\n",
              "2   als  Alemannic German  ...  (ursprünglich nur Elsässisch)             NaN\n",
              "3   amh           Amharic  ...                            NaN             NaN\n",
              "4   ang      Old English   ...                 (ca. 450-1100)  Angelsächsisch\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhgcMSRxPygW"
      },
      "source": [
        "Récupère les labels de langue qui sont utilisés dans \"y_train.txt\" et \"y_test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCsLh1QHABbL",
        "outputId": "2897394d-e3c3-42cf-c65c-877536449602"
      },
      "source": [
        "lang_labels = list(labels_df[labels_df['English'].isin(langs)]['Label'])\n",
        "lang_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eng', 'fra', 'hin', 'lzh', 'spa', 'zho']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XccdPYUheIRQ"
      },
      "source": [
        "Récupère le nom complet des langues que nous utiliserons. C'est ce qui nous permettra de comparer le résultat des prédiction d'**Azure Text Analytics** avec ce qui est attendu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tx1V5jVQVZK",
        "outputId": "b895a6d8-87b9-45a7-a0d1-b1864f876a1d"
      },
      "source": [
        "lang_names = (list(labels_df[labels_df['English'].isin(langs)]['English']))\n",
        "lang_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['English',\n",
              " 'French',\n",
              " 'Hindi',\n",
              " 'Literary Chinese',\n",
              " 'Spanish',\n",
              " 'Standard Chinese']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS5k44-kQ6IF"
      },
      "source": [
        "Cette fonction extrait les paragraphes et des labels de langue pour les langues de la liste **langs**, et en fait un DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7V4OUSIRQqK"
      },
      "source": [
        "def read_file(x_file, y_file):\n",
        "  # Récupère le contenu de  'y_file' dans un dataframe\n",
        "  y_df = pd.read_csv(y_file, header=None)\n",
        "  # y_df a une seule colonne qui s'appelera \"labels\"\n",
        "  y_df.columns = [\"Label\"]\n",
        "\n",
        "  # Récupère le contenu de 'x_file' dans une liste de strings\n",
        "  with open(x_file, encoding=\"utf8\") as f:\n",
        "      x_pars = f.readlines()\n",
        "\n",
        "  # Enlève les espaces et autres caractères invisibles (comme '\\n') au début et à la fin des strings\n",
        "  x_pars = [t.strip() for t in x_pars]\n",
        "  # Convertit la liste en dataframe à une seule colonne: 'Paragraphe'\n",
        "  x_df = pd.DataFrame(x_pars, columns=['Paragraph']) \n",
        "  # Ne garde que les paragraphes des langues connues (et enlève les autres)\n",
        "  x_df = x_df[y_df['Label'].isin(lang_labels)]\n",
        "  # Ne garde que ces langues dans lang_labels\n",
        "  y_df = y_df[y_df['Label'].isin(lang_labels)]\n",
        "\n",
        "  return (x_df, y_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhIH6wXxfgBY"
      },
      "source": [
        "Vérifions que le nombre de lignes correspond:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm80Jp_OgjRR",
        "outputId": "d44582f9-4954-47b6-8a05-e35fad2de8d4"
      },
      "source": [
        "lang_df, label_df = read_file(x_test_file, y_test_file)\n",
        "lang_df.shape, label_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3000, 1), (3000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KZFJlsTxhHCh",
        "outputId": "fd17093f-5e09-4afb-9f44-a2ca543e9f36"
      },
      "source": [
        "lang_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Paragraph</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>大都会区有它自己的当地路边快餐口味，包括瓦达帕夫（蓬松面包劈开一半，填入锅贴）、潘尼普里（油...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>La ciudad de San Cristóbal es sede del Hospita...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>Les supporters de l'ASM Clermont Auvergne ont ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>吳廷琰,越南首相，越南共和國總統也。成泰十三年誕於廣平。父廷可，宮監大臣。廷琰其三子也。年十...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>Anton (or Antonius) Maria Schyrleus (also Schy...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Paragraph\n",
              "42  大都会区有它自己的当地路边快餐口味，包括瓦达帕夫（蓬松面包劈开一半，填入锅贴）、潘尼普里（油...\n",
              "50  La ciudad de San Cristóbal es sede del Hospita...\n",
              "54  Les supporters de l'ASM Clermont Auvergne ont ...\n",
              "57  吳廷琰,越南首相，越南共和國總統也。成泰十三年誕於廣平。父廷可，宮監大臣。廷琰其三子也。年十...\n",
              "68  Anton (or Antonius) Maria Schyrleus (also Schy..."
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nDMoq0C5ph92",
        "outputId": "15f2c365-e7b6-467d-9aa1-ae439c6c3192"
      },
      "source": [
        "label_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>zho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>spa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>fra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>lzh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>eng</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label\n",
              "42   zho\n",
              "50   spa\n",
              "54   fra\n",
              "57   lzh\n",
              "68   eng"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB2AEu9ssIPc"
      },
      "source": [
        "la fonction **verif_predictions()** va récupérer les paragraphes à tester, constituer un payload qui contiendra au maximum 1000 paragraphes, avec au plus 5120 caractères par paragraphe (limites Azure Cognitive Service - Text Analytics), et renvoyer un dataframe avec les colonnes suivantes :\n",
        "- **id** : l'identifiant du texte\n",
        "- **Paragraphe** : le paragraphe qui a été envoyé\n",
        "- **expected** : la langue attendue\n",
        "- **predicted** : la langue qui a été détectée\n",
        "- **confidenceScore** : le score de confiance de la prédiction renvoyé par l'API\n",
        "- **valid** : un booléen qui est **True** si **expected** et **predicted** ont la même valeur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-Bx864ipZ9s"
      },
      "source": [
        "def verif_predictions():\n",
        "    # structure du resultat des tests : id, paragraphe, expected, predicted, confidence score, valid\n",
        "    lang_df, label_df = read_file(x_test_file, y_test_file)\n",
        "    lang_df.index.name = \"id\"\n",
        "    label_df.index.name = \"id\"\n",
        "    last_index = lang_df.index[-1]\n",
        "    max_doc_amount = Language_Detection().MAX_DOC_AMOUNT\n",
        "    max_txt_size = Language_Detection().MAX_TXT_SIZE\n",
        "    test_results = []\n",
        "    doc_amount = 0\n",
        "    payload = {}\n",
        "    payload[\"documents\"] = []\n",
        "    \n",
        "    for i in lang_df.index:\n",
        "        if(doc_amount < max_doc_amount):\n",
        "            doc_amount += 1\n",
        "        # lang par est le paragraphe de langue qui a été extrait\n",
        "        lang_par = lang_df[\"Paragraph\"][i]\n",
        "        if(len(lang_par) > max_txt_size):\n",
        "            lang_par = lang_par[:max_txt_size - 1]\n",
        "        document = {}\n",
        "        document[\"id\"] = i\n",
        "        document[\"text\"] = lang_par\n",
        "        payload[\"documents\"].append(document)\n",
        "        if((doc_amount == max_doc_amount) or (i == last_index)):\n",
        "            res = json.loads(Language_Detection().call_api(json.dumps(payload)))\n",
        "            print(res)\n",
        "            for document in res[\"documents\"]:\n",
        "                test_results.append([document[\"id\"], \n",
        "                                 document[\"detectedLanguage\"][\"name\"], \n",
        "                                 document[\"detectedLanguage\"][\"confidenceScore\"]])\n",
        "            payload = {}\n",
        "            payload[\"documents\"] = []\n",
        "            doc_amount= 0\n",
        "            if(i == last_index):\n",
        "                df_results = pd.DataFrame(test_results, columns=[\"id\", \"Predicted\", \"confidenceScore\"])\n",
        "                df_results.set_index(\"id\", inplace=True)\n",
        "                df_results.index = df_results.index.astype(int)\n",
        "                \n",
        "                \n",
        "                df_final = pd.merge(df_results, pd.merge(lang_df, label_df, left_index=True, right_index=True), left_index=True, right_index=True)\n",
        "                df_final = pd.merge(df_final, labels_df, on=\"Label\", how=\"left\").set_index(df_final.index)\n",
        "                df_final = df_final.drop([\"Wiki Code\", \"ISO 369-3\", \"German\", \"Language family\", \"Writing system\", \"Remarks\", \"Synonyms\", \"Label\"], axis=1)\n",
        "                df_final = df_final.rename(columns={\"English\": \"Expected\"})\n",
        "                df_final = df_final[[\"Paragraph\", \"Predicted\", \"Expected\", \"confidenceScore\"]]\n",
        "                # dans le dataset wili-2018, les langues chinoises n'ont pas le même nom\n",
        "                # remplaçons donc les noms des langues de la colonne \"Expected\" par ceux correspondants pour les langues chinoises\n",
        "                df_final.replace({\"Standard Chinese\": \"Chinese Simplified\"}, regex=True)\n",
        "                df_final.replace({\"Literary Chinese\": \"Chinese Traditional\"}, regex=True)\n",
        "                df_final[\"Valid\"] = (df_final[\"Predicted\"] == df_final[\"Expected\"])\n",
        "                return df_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhhLrcv-0pSU"
      },
      "source": [
        "df_results = verif_predictions()\n",
        "df_results.to_csv(r'./results.csv')\n",
        "dg_results.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOAs1aRdMwnq"
      },
      "source": [
        "df_results.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj7fyq3nqDg0"
      },
      "source": [
        "Regardons quel est le pourcentage de bonnes et de mauvaises prédictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0KJ880I4MxL"
      },
      "source": [
        "df_results[\"Valid\"].value_counts(normalize=True) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deAuZu85xIdz"
      },
      "source": [
        "regardons l'ensemble des prédictions erronées :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubH7AxQZxRLV"
      },
      "source": [
        "df_results[df_results[\"Valid\"] == False]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbQbeo9XuVS9"
      },
      "source": [
        "on peut aussi trier les résultats par langue :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "6t0zafWYuwqb",
        "outputId": "25baca91-f15d-4ce2-9988-e9f04c4ffdd2"
      },
      "source": [
        "df_results.groupby(\"Expected\")[\"Valid\"].value_counts(normalize=True) * 100"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-634806629245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYD_boInyqrO"
      },
      "source": [
        "et afficher un histogramme des résultats par langue :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "r99QGa7cxQQZ",
        "outputId": "9a0e75d0-dfff-4e5a-9e31-eae8aec17245"
      },
      "source": [
        "df_results[\"Valid\"].hist(by=df_results[\"Expected\"])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2f171a307ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Expected\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBxuSINdy8_x"
      },
      "source": [
        "df_results.groupby(\"Expected\").count()[\"Valid\"].hist() # produit le même résultat que la cellule précédente"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}